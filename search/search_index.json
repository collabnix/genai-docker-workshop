{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GenAI Docker Workshop","text":"<p>Welcome to the GenAI Docker Workshop! This comprehensive workshop is designed to help you understand and implement Generative AI solutions using Docker containers.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Fundamentals of Generative AI and its relationship with Docker</li> <li>Understanding the GenAI stack architecture</li> <li>Common challenges in deploying GenAI applications</li> <li>Working with Model Runner for efficient model deployments</li> <li>Utilizing the MCP Toolkit for container management</li> <li>Hands-on implementation of GenAI applications with Docker</li> </ul>"},{"location":"#workshop-structure","title":"Workshop Structure","text":"<p>This workshop is structured into several sections:</p> <ol> <li>Getting Started: Basic introduction and setup</li> <li>GenAI Fundamentals: Core concepts and architecture</li> <li>Docker for GenAI: How Docker enhances GenAI workflows</li> <li>Labs: Hands-on exercises to implement what you've learned</li> <li>Resources: Additional materials to deepen your knowledge</li> </ol>"},{"location":"#who-is-this-for","title":"Who Is This For?","text":"<p>This workshop is ideal for:</p> <ul> <li>DevOps engineers looking to deploy GenAI models</li> <li>Data scientists wanting to containerize their models</li> <li>Software developers integrating GenAI capabilities</li> <li>IT professionals managing GenAI infrastructure</li> <li>Anyone interested in the intersection of containers and AI</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Basic knowledge of:</p> <ul> <li>Docker fundamentals</li> <li>Command line operations</li> <li>Python (for lab exercises)</li> </ul> <p>Detailed prerequisites can be found in the Prerequisites section.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to begin? Head over to the Introduction section to get started!</p> <p>Created with ?? by Collabnix Community</p>"},{"location":"docker-genai/best-practices/","title":"Best Practices for GenAI with Docker","text":"<p>This page outlines best practices for deploying and managing GenAI applications using Docker.</p>"},{"location":"docker-genai/best-practices/#development-best-practices","title":"Development Best Practices","text":""},{"location":"docker-genai/best-practices/#separate-development-and-inference-environments","title":"Separate Development and Inference Environments","text":"<ul> <li>Use Multi-stage Builds: Create separate build stages for development and inference</li> <li>Development Container: Include training libraries and debugging tools</li> <li>Inference Container: Streamlined with only necessary dependencies for production</li> </ul> <pre><code># Development stage\nFROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime AS development\nRUN pip install jupyter matplotlib scikit-learn pandas\n# Add development tools\n\n# Inference stage\nFROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime AS inference\nCOPY --from=development /app/model /app/model\n# Only include inference requirements\n</code></pre>"},{"location":"docker-genai/best-practices/#efficient-model-management","title":"Efficient Model Management","text":"<ul> <li>Separate Model from Code: Store models in volumes or object storage</li> <li>Version Control for Models: Use tagging and versioning for model artifacts</li> <li>Model Registry: Consider using a model registry like MLflow or Model Runner</li> </ul>"},{"location":"docker-genai/best-practices/#development-workflow","title":"Development Workflow","text":"<ul> <li>Local Testing: Test models locally before containerizing</li> <li>Consistent Environments: Use the same container for local development and CI/CD</li> <li>Integration Testing: Test the containerized model with realistic workloads</li> </ul>"},{"location":"docker-genai/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"docker-genai/best-practices/#container-level-optimizations","title":"Container-Level Optimizations","text":"<ul> <li>Resource Limits: Set appropriate CPU, memory, and GPU limits</li> <li>CPU Pinning: Consider CPU pinning for critical workloads</li> <li>NUMA Awareness: For multi-socket servers, use NUMA-aware configurations</li> </ul> <pre><code>docker run --cpuset-cpus=\"0-3\" --memory=8g --gpus device=0 my-genai-model\n</code></pre>"},{"location":"docker-genai/best-practices/#docker-storage-considerations","title":"Docker Storage Considerations","text":"<ul> <li>Choose Appropriate Storage Driver: Consider overlay2 for better performance</li> <li>Volume Mounting: Use volumes for model storage rather than copying into containers</li> <li>Tmpfs for Ephemeral Data: Use tmpfs mounts for temporary data</li> </ul>"},{"location":"docker-genai/best-practices/#network-optimization","title":"Network Optimization","text":"<ul> <li>Placement: Place related services in the same Docker network</li> <li>Bridge vs Host: Consider host networking for maximum performance</li> <li>Direct Container Communication: Use Docker DNS for service discovery</li> </ul>"},{"location":"docker-genai/best-practices/#deployment-strategies","title":"Deployment Strategies","text":""},{"location":"docker-genai/best-practices/#scaling-approaches","title":"Scaling Approaches","text":"<ul> <li>Horizontal Scaling: Deploy multiple container instances behind a load balancer</li> <li>Auto-scaling: Use orchestration platforms to scale based on metrics</li> <li>Resource-based Scaling: Scale based on CPU, memory, or GPU utilization</li> </ul>"},{"location":"docker-genai/best-practices/#high-availability","title":"High Availability","text":"<ul> <li>Health Checks: Implement robust container health checks</li> <li>Graceful Termination: Handle SIGTERM properly for clean shutdowns</li> <li>Rolling Updates: Use rolling updates for zero-downtime deployments</li> </ul>"},{"location":"docker-genai/best-practices/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Metrics Collection: Export metrics from containers (Prometheus format)</li> <li>Log Management: Implement structured logging with consistent formats</li> <li>Tracing: Add distributed tracing for complex deployments</li> </ul>"},{"location":"docker-genai/best-practices/#security-best-practices","title":"Security Best Practices","text":""},{"location":"docker-genai/best-practices/#container-security","title":"Container Security","text":"<ul> <li>Minimal Base Images: Use minimal or distroless base images</li> <li>Non-root Users: Run containers as non-root users</li> <li>Read-only Filesystem: Mount filesystems as read-only where possible</li> </ul> <pre><code>FROM python:3.10-slim\n\n# Create a non-root user\nRUN groupadd -r modeluser &amp;&amp; useradd -r -g modeluser modeluser\nWORKDIR /app\n\n# Install dependencies and copy application\nCOPY --chown=modeluser:modeluser . .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Switch to non-root user\nUSER modeluser\n\nENTRYPOINT [\"python\", \"serve_model.py\"]\n</code></pre>"},{"location":"docker-genai/best-practices/#secrets-management","title":"Secrets Management","text":"<ul> <li>Environment Variables: Avoid secrets in environment variables</li> <li>Docker Secrets: Use Docker secrets or Kubernetes secrets</li> <li>Secret Management Tools: Consider tools like HashiCorp Vault</li> </ul>"},{"location":"docker-genai/best-practices/#image-security","title":"Image Security","text":"<ul> <li>Vulnerability Scanning: Scan images regularly</li> <li>Content Trust: Use Docker Content Trust for image signing</li> <li>Registry Security: Secure your Docker registry access</li> </ul>"},{"location":"docker-genai/best-practices/#genai-specific-best-practices","title":"GenAI-Specific Best Practices","text":""},{"location":"docker-genai/best-practices/#model-optimization","title":"Model Optimization","text":"<ul> <li>Quantization: Use quantized models when possible (INT8, FP16)</li> <li>Model Pruning: Remove unnecessary weights or layers</li> <li>TensorRT/ONNX: Convert models to optimized formats</li> </ul>"},{"location":"docker-genai/best-practices/#serving-architecture","title":"Serving Architecture","text":"<ul> <li>Batch Processing: Implement batching for better throughput</li> <li>Request Queuing: Add queuing mechanisms for traffic spikes</li> <li>Request Priority: Implement priority queues for important requests</li> </ul>"},{"location":"docker-genai/best-practices/#caching-strategies","title":"Caching Strategies","text":"<ul> <li>Response Caching: Cache common responses</li> <li>Embedding Caching: Cache embeddings for frequently accessed content</li> <li>Tiered Caching: Implement memory and disk-based caching</li> </ul>"},{"location":"docker-genai/best-practices/#orchestration-best-practices","title":"Orchestration Best Practices","text":""},{"location":"docker-genai/best-practices/#kubernetes-for-genai","title":"Kubernetes for GenAI","text":"<ul> <li>Custom Resources: Consider custom resources for model deployment</li> <li>GPU Scheduling: Configure proper GPU limits and scheduling</li> <li>StatefulSets: Use StatefulSets for stateful model servers</li> </ul>"},{"location":"docker-genai/best-practices/#compose-for-development","title":"Compose for Development","text":"<ul> <li>Development Environment: Create comprehensive development environments</li> <li>Service Definition: Define all dependent services</li> <li>Volume Mapping: Map source code for rapid iteration</li> </ul>"},{"location":"docker-genai/best-practices/#swarm-for-simple-deployments","title":"Swarm for Simple Deployments","text":"<ul> <li>Overlay Networks: Use overlay networks for multi-host deployments</li> <li>Secrets Management: Utilize Docker Swarm secrets</li> <li>Rolling Updates: Configure update policies for zero-downtime deployments</li> </ul>"},{"location":"docker-genai/best-practices/#conclusion","title":"Conclusion","text":"<p>Following these best practices will help you build reliable, efficient, and secure GenAI applications with Docker. The key is to:</p> <ol> <li>Optimize for both development and production workflows</li> <li>Carefully manage resources, especially GPU and memory</li> <li>Implement proper security controls</li> <li>Design for scalability and resilience</li> <li>Monitor and observe system performance</li> </ol> <p>By combining Docker's containerization capabilities with these GenAI-specific best practices, you can create deployments that are both powerful and maintainable.</p>"},{"location":"docker-genai/mcp-toolkit/","title":"MCP Toolkit","text":"<p>The Model Container Platform (MCP) Toolkit is Docker's comprehensive solution for managing the entire lifecycle of containerized AI models. It provides tools and frameworks to simplify the development, deployment, and management of GenAI applications.</p>"},{"location":"docker-genai/mcp-toolkit/#what-is-the-mcp-toolkit","title":"What is the MCP Toolkit?","text":"<p>The MCP Toolkit is a suite of tools designed to address the unique challenges of working with containerized AI models. It sits on top of Docker's core containerization technology and provides specialized capabilities for AI workloads.</p> <p>Key aspects of the MCP Toolkit include:</p> <ul> <li>Model Lifecycle Management: Tools for managing models from development to retirement</li> <li>Deployment Automation: Streamlined workflows for deploying models to production</li> <li>Resource Optimization: Intelligent allocation and management of computing resources</li> <li>Monitoring and Observability: Comprehensive visibility into model performance</li> <li>Integration Capabilities: Connecting models with other systems and data sources</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#components-of-mcp-toolkit","title":"Components of MCP Toolkit","text":""},{"location":"docker-genai/mcp-toolkit/#mcp-cli","title":"MCP CLI","text":"<p>The command-line interface for interacting with the MCP platform:</p> <ul> <li>Model Management: Commands for registering, updating, and deploying models</li> <li>Environment Management: Tools for configuring and managing deployment environments</li> <li>Monitoring: Commands for checking status and performance</li> <li>Automation: Scripting capabilities for workflow automation</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#model-registry","title":"Model Registry","text":"<p>Central repository for managing model artifacts:</p> <ul> <li>Version Control: Tracking multiple versions of models</li> <li>Metadata Management: Storing and retrieving model metadata</li> <li>Access Control: Managing who can deploy or modify models</li> <li>Dependency Tracking: Managing relationships between models and dependencies</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#deployment-manager","title":"Deployment Manager","text":"<p>Tools for deploying models to various environments:</p> <ul> <li>Environment Templates: Pre-configured environments for different model types</li> <li>Scaling Policies: Rules for automatically scaling model instances</li> <li>Rolling Updates: Zero-downtime updates of deployed models</li> <li>Canary Deployments: Gradual rollout of new model versions</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#resource-optimizer","title":"Resource Optimizer","text":"<p>Intelligent management of computing resources:</p> <ul> <li>GPU Allocation: Optimized allocation of GPU resources</li> <li>Memory Management: Efficient use of system memory</li> <li>Cost Optimization: Balancing performance and resource costs</li> <li>Resource Sharing: Maximizing utilization across multiple models</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#monitoring-framework","title":"Monitoring Framework","text":"<p>Comprehensive visibility into model operations:</p> <ul> <li>Performance Metrics: Tracking throughput, latency, and resource usage</li> <li>Quality Metrics: Monitoring the quality of model outputs</li> <li>Alerting: Notifications when metrics exceed thresholds</li> <li>Visualization: Dashboards for monitoring model performance</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#using-the-mcp-toolkit","title":"Using the MCP Toolkit","text":""},{"location":"docker-genai/mcp-toolkit/#basic-workflow","title":"Basic Workflow","text":"<p>The typical workflow with MCP Toolkit includes:</p> <ol> <li> <p>Model Registration:    <pre><code>mcp model register --name \"gpt-model\" --path ./model-artifacts --version 1.0\n</code></pre></p> </li> <li> <p>Environment Configuration:    <pre><code>mcp environment create --name \"production\" --type gpu --resources \"gpu=1,memory=32Gi\"\n</code></pre></p> </li> <li> <p>Model Deployment:    <pre><code>mcp deploy --model \"gpt-model:1.0\" --environment \"production\" --replicas 3\n</code></pre></p> </li> <li> <p>Monitoring Deployment:    <pre><code>mcp status --deployment \"gpt-model-production\"\n</code></pre></p> </li> <li> <p>Scaling Adjustment:    <pre><code>mcp scale --deployment \"gpt-model-production\" --replicas 5\n</code></pre></p> </li> </ol>"},{"location":"docker-genai/mcp-toolkit/#advanced-features","title":"Advanced Features","text":""},{"location":"docker-genai/mcp-toolkit/#ab-testing","title":"A/B Testing","text":"<p>Setting up A/B testing between model versions:</p> <pre><code>mcp ab-test create \\\n  --name \"gpt-model-test\" \\\n  --modelA \"gpt-model:1.0\" \\\n  --modelB \"gpt-model:1.1\" \\\n  --traffic-split \"80:20\" \\\n  --environment \"production\"\n</code></pre>"},{"location":"docker-genai/mcp-toolkit/#automated-workflows","title":"Automated Workflows","text":"<p>Creating automated workflows for model updates:</p> <pre><code>mcp workflow create \\\n  --name \"model-update-flow\" \\\n  --steps \"validate,deploy-canary,test-canary,deploy-full\" \\\n  --model \"gpt-model\" \\\n  --environment \"production\"\n</code></pre>"},{"location":"docker-genai/mcp-toolkit/#performance-profiling","title":"Performance Profiling","text":"<p>Analyzing model performance for optimization:</p> <pre><code>mcp profile --deployment \"gpt-model-production\" --duration 30m\n</code></pre>"},{"location":"docker-genai/mcp-toolkit/#integration-with-docker-ecosystem","title":"Integration with Docker Ecosystem","text":"<p>The MCP Toolkit integrates seamlessly with the broader Docker ecosystem:</p>"},{"location":"docker-genai/mcp-toolkit/#docker-compose-integration","title":"Docker Compose Integration","text":"<p>Using Docker Compose with MCP:</p> <pre><code># docker-compose.yml\nversion: '3'\nservices:\n  model-service:\n    image: ${MCP_REGISTRY}/gpt-model:1.0\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    environment:\n      - MCP_MODEL_CONFIG=/configs/model-config.json\n    volumes:\n      - ./configs:/configs\n</code></pre>"},{"location":"docker-genai/mcp-toolkit/#kubernetes-integration","title":"Kubernetes Integration","text":"<p>Deploying MCP-managed models to Kubernetes:</p> <pre><code>mcp kubernetes deploy \\\n  --model \"gpt-model:1.0\" \\\n  --namespace \"ai-services\" \\\n  --service-type LoadBalancer \\\n  --replicas 3\n</code></pre>"},{"location":"docker-genai/mcp-toolkit/#cicd-integration","title":"CI/CD Integration","text":"<p>Integrating MCP with CI/CD pipelines:</p> <pre><code># Example GitHub Actions workflow\nname: Deploy Model\non:\n  push:\n    branches: [main]\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Install MCP CLI\n        run: |\n          curl -sL https://get.mcp.docker.com | bash\n      - name: Deploy Model\n        run: |\n          mcp deploy --model \"gpt-model:${{ github.sha }}\" --environment \"production\"\n</code></pre>"},{"location":"docker-genai/mcp-toolkit/#best-practices-for-mcp-toolkit","title":"Best Practices for MCP Toolkit","text":""},{"location":"docker-genai/mcp-toolkit/#organizational-structure","title":"Organizational Structure","text":"<ul> <li>Model Naming Conventions: Establish consistent naming for models and versions</li> <li>Environment Separation: Clearly separate development, testing, and production environments</li> <li>Access Controls: Implement appropriate permissions for different team roles</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#deployment-strategy","title":"Deployment Strategy","text":"<ul> <li>Incremental Rollouts: Use canary or blue/green deployments for critical models</li> <li>Automated Testing: Implement automated tests before full deployment</li> <li>Rollback Planning: Have clear procedures for rolling back problematic deployments</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#resource-management","title":"Resource Management","text":"<ul> <li>Right-sizing Resources: Allocate appropriate resources based on model requirements</li> <li>Scaling Policies: Set appropriate auto-scaling thresholds</li> <li>Cost Monitoring: Regularly review resource utilization and costs</li> </ul>"},{"location":"docker-genai/mcp-toolkit/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<ul> <li>Comprehensive Monitoring: Track both technical and business metrics</li> <li>Regular Updates: Keep models and dependencies up to date</li> <li>Performance Optimization: Regularly analyze and optimize model performance</li> </ul> <p>In the following labs, we'll explore hands-on examples of using the MCP Toolkit for real-world GenAI applications.</p>"},{"location":"docker-genai/model-runner/","title":"Docker Model Runner","text":"<p>Model Runner is Docker's specialized tool for running AI models efficiently in containerized environments. It provides optimized infrastructure for deploying and serving GenAI models with improved performance, scalability, and operational simplicity.</p>"},{"location":"docker-genai/model-runner/#what-is-model-runner","title":"What is Model Runner?","text":"<p>Model Runner is a purpose-built runtime environment designed specifically for running inference workloads of large language models (LLMs) and other generative AI models. It includes:</p> <ul> <li>Optimized Container Runtime: Configured for efficient model serving</li> <li>Performance Optimizations: Pre-configured settings for optimal throughput and latency</li> <li>Resource Management: Intelligent allocation of CPU, memory, and GPU resources</li> <li>Monitoring Capabilities: Built-in metrics and observability features</li> <li>Standardized APIs: Consistent interfaces for model interaction</li> </ul>"},{"location":"docker-genai/model-runner/#key-features-of-model-runner","title":"Key Features of Model Runner","text":""},{"location":"docker-genai/model-runner/#optimized-inference","title":"Optimized Inference","text":"<p>Model Runner includes several optimizations for model inference:</p> <ul> <li>Dynamic Batching: Intelligently combines multiple requests for better throughput</li> <li>Quantization Support: First-class support for quantized models (INT8, FP16)</li> <li>Memory Management: Efficient handling of model weights in memory</li> <li>Caching Mechanisms: Caching for frequently accessed model components</li> </ul>"},{"location":"docker-genai/model-runner/#model-format-support","title":"Model Format Support","text":"<p>Model Runner supports multiple model formats:</p> <ul> <li>ONNX: Open Neural Network Exchange format</li> <li>PyTorch: Models from the PyTorch ecosystem</li> <li>TensorFlow: TensorFlow model formats</li> <li>Hugging Face Transformers: Direct support for Hugging Face models</li> <li>Custom Formats: Extensible for proprietary model formats</li> </ul>"},{"location":"docker-genai/model-runner/#resource-optimization","title":"Resource Optimization","text":"<p>Model Runner intelligently manages computational resources:</p> <ul> <li>GPU Utilization: Maximizing the use of available GPU capacity</li> <li>CPU Offloading: Shifting appropriate workloads to CPU when beneficial</li> <li>Memory Optimization: Minimizing memory footprint of large models</li> <li>Dynamic Scaling: Adjusting resources based on demand</li> </ul>"},{"location":"docker-genai/model-runner/#deployment-flexibility","title":"Deployment Flexibility","text":"<p>Model Runner offers various deployment options:</p> <ul> <li>Standalone Deployment: Running as a single container</li> <li>Orchestrated Deployment: Integration with Kubernetes and Docker Swarm</li> <li>Multi-model Hosting: Running multiple models in the same environment</li> <li>Edge Deployment: Optimized configurations for edge devices</li> </ul>"},{"location":"docker-genai/model-runner/#model-runner-architecture","title":"Model Runner Architecture","text":"<p>Model Runner is built with a modular architecture consisting of several key components:</p>"},{"location":"docker-genai/model-runner/#core-components","title":"Core Components","text":"<ol> <li>Model Server: The central component that handles inference requests</li> <li>Model Repository: Manages model artifacts and versions</li> <li>Inference Optimizer: Applies optimizations to improve performance</li> <li>Resource Manager: Allocates and monitors resource usage</li> <li>API Gateway: Provides standardized interfaces for model access</li> </ol>"},{"location":"docker-genai/model-runner/#workflow","title":"Workflow","text":"<p>The typical workflow for Model Runner includes:</p> <ol> <li>Model Registration: Registering a model with the Model Repository</li> <li>Deployment Configuration: Setting performance and scaling parameters</li> <li>Container Deployment: Launching the containerized model</li> <li>Request Handling: Processing inference requests</li> <li>Monitoring and Scaling: Tracking performance and adjusting resources</li> </ol>"},{"location":"docker-genai/model-runner/#using-model-runner","title":"Using Model Runner","text":""},{"location":"docker-genai/model-runner/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example of deploying a model with Model Runner:</p> <pre><code># Pull the Model Runner image\ndocker pull docker/genai-model-runner:latest\n\n# Run a model with Model Runner\ndocker run -d -p 8000:8000 \\\n  --gpus all \\\n  -v /path/to/models:/models \\\n  docker/genai-model-runner:latest \\\n  --model-name \"llama2-7b\" \\\n  --model-path \"/models/llama2-7b\"\n</code></pre>"},{"location":"docker-genai/model-runner/#advanced-configuration","title":"Advanced Configuration","text":"<p>Model Runner offers numerous configuration options:</p> <pre><code># Run with advanced configuration\ndocker run -d -p 8000:8000 \\\n  --gpus all \\\n  -v /path/to/models:/models \\\n  -e MAX_BATCH_SIZE=8 \\\n  -e DYNAMIC_BATCHING=true \\\n  -e QUANTIZATION=int8 \\\n  -e MAX_CONCURRENT_REQUESTS=32 \\\n  docker/genai-model-runner:latest \\\n  --model-name \"llama2-7b\" \\\n  --model-path \"/models/llama2-7b\" \\\n  --max-seq-len 2048 \\\n  --enable-metrics\n</code></pre>"},{"location":"docker-genai/model-runner/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>Model Runner integrates well with Kubernetes:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: model-runner-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: model-runner\n  template:\n    metadata:\n      labels:\n        app: model-runner\n    spec:\n      containers:\n      - name: model-runner\n        image: docker/genai-model-runner:latest\n        args:\n        - \"--model-name=llama2-7b\"\n        - \"--model-path=/models/llama2-7b\"\n        - \"--enable-metrics\"\n        resources:\n          limits:\n            nvidia.com/gpu: 1\n        volumeMounts:\n        - name: model-volume\n          mountPath: /models\n      volumes:\n      - name: model-volume\n        persistentVolumeClaim:\n          claimName: model-pvc\n</code></pre>"},{"location":"docker-genai/model-runner/#performance-monitoring","title":"Performance Monitoring","text":"<p>Model Runner includes built-in monitoring capabilities:</p>"},{"location":"docker-genai/model-runner/#metrics-exposed","title":"Metrics Exposed","text":"<ul> <li>Throughput: Requests processed per second</li> <li>Latency: Response time percentiles (p50, p95, p99)</li> <li>GPU Utilization: Percentage of GPU capacity used</li> <li>Memory Usage: Model memory consumption</li> <li>Queue Depth: Number of requests waiting for processing</li> </ul>"},{"location":"docker-genai/model-runner/#integration-with-monitoring-tools","title":"Integration with Monitoring Tools","text":"<p>Model Runner metrics can be collected by:</p> <ul> <li>Prometheus: Direct scraping of exposed metrics</li> <li>Grafana: Visualization of performance data</li> <li>Docker Desktop: Integration with Docker metrics</li> <li>Custom Solutions: Via exposed API endpoints</li> </ul>"},{"location":"docker-genai/model-runner/#best-practices-for-model-runner","title":"Best Practices for Model Runner","text":""},{"location":"docker-genai/model-runner/#resource-allocation","title":"Resource Allocation","text":"<ul> <li>Start with resource settings recommended for your model size</li> <li>Monitor actual usage and adjust accordingly</li> <li>Consider dedicated instances for large models</li> </ul>"},{"location":"docker-genai/model-runner/#performance-tuning","title":"Performance Tuning","text":"<ul> <li>Experiment with different batch sizes for your workload</li> <li>Test various quantization levels for your accuracy requirements</li> <li>Adjust concurrency settings based on request patterns</li> </ul>"},{"location":"docker-genai/model-runner/#deployment-strategy","title":"Deployment Strategy","text":"<ul> <li>Use rolling updates for model version changes</li> <li>Implement health checks for reliable operations</li> <li>Consider blue/green deployments for zero-downtime updates</li> </ul> <p>In the next lab, we'll perform hands-on exercises with Model Runner to see these concepts in action.</p>"},{"location":"docker-genai/overview/","title":"Docker for GenAI: Overview","text":"<p>Docker provides a powerful platform for developing, deploying, and managing generative AI applications. This section explores how Docker specifically enhances GenAI workflows and the specialized tools it offers for AI workloads.</p>"},{"location":"docker-genai/overview/#why-docker-is-ideal-for-genai","title":"Why Docker is Ideal for GenAI","text":"<p>GenAI applications have unique requirements that Docker's containerization approach addresses effectively:</p>"},{"location":"docker-genai/overview/#environment-consistency","title":"Environment Consistency","text":"<ul> <li>Dependency Management: GenAI models often require complex dependencies that can conflict with other applications or system libraries. Docker containers package all dependencies together, eliminating \"it works on my machine\" problems.</li> <li>Reproducibility: Docker ensures the same environment across development, testing, and production, which is critical for the consistent behavior of AI models.</li> <li>Version Control: Docker allows precise versioning of both the model and its runtime environment, enabling reliable rollbacks and A/B testing.</li> </ul>"},{"location":"docker-genai/overview/#resource-efficiency","title":"Resource Efficiency","text":"<ul> <li>Isolation with Low Overhead: Docker provides isolation without the performance overhead of traditional virtual machines, maximizing resources available for compute-intensive AI workloads.</li> <li>Right-sizing Resources: Container-level resource constraints allow fine-grained allocation of CPU, memory, and GPU resources to match specific model needs.</li> <li>Efficient Scaling: Docker enables horizontal scaling by spinning up additional container instances as demand increases.</li> </ul>"},{"location":"docker-genai/overview/#workflow-improvement","title":"Workflow Improvement","text":"<ul> <li>Developer Experience: Docker simplifies the development workflow, allowing data scientists to focus on model development rather than infrastructure.</li> <li>CI/CD Integration: Containerization enables automated testing and deployment of models through established CI/CD pipelines.</li> <li>Multi-stage Builds: Docker's multi-stage builds allow separating heavy training environments from lightweight inference environments.</li> </ul>"},{"location":"docker-genai/overview/#docker-components-for-genai","title":"Docker Components for GenAI","text":"<p>Docker offers several specialized components designed specifically for AI workloads:</p>"},{"location":"docker-genai/overview/#core-docker-features-for-genai","title":"Core Docker Features for GenAI","text":"<ul> <li>GPU Support: Native support for NVIDIA GPUs through the NVIDIA Container Toolkit, essential for efficient model training and inference.</li> <li>Multi-architecture Images: Support for different CPU architectures, enabling deployment across diverse infrastructure.</li> <li>Volume Management: Efficient handling of large model weights and datasets through Docker volumes.</li> <li>Networking Capabilities: Advanced networking for complex microservices architectures common in GenAI applications.</li> </ul>"},{"location":"docker-genai/overview/#specialized-ai-tools","title":"Specialized AI Tools","text":"<ul> <li>Model Runner: Docker's optimized runtime environment for deploying AI models with high performance and efficiency.</li> <li>MCP Toolkit: The Model Container Platform Toolkit for comprehensive management of containerized AI models.</li> <li>AI Extensions: Docker extensions specifically designed for AI workflow enhancement.</li> </ul>"},{"location":"docker-genai/overview/#common-genai-deployment-patterns-with-docker","title":"Common GenAI Deployment Patterns with Docker","text":"<p>Several deployment patterns have emerged as best practices for GenAI applications with Docker:</p>"},{"location":"docker-genai/overview/#single-container-deployment","title":"Single-container Deployment","text":"<ul> <li>Simple deployment of a single model in a container</li> <li>Suitable for smaller models or low-traffic applications</li> <li>Example: A containerized BERT model for sentiment analysis</li> </ul>"},{"location":"docker-genai/overview/#microservices-architecture","title":"Microservices Architecture","text":"<ul> <li>Breaking the GenAI application into multiple specialized containers</li> <li>Each container handles a specific function (preprocessing, inference, post-processing)</li> <li>Benefits: Better scalability, easier maintenance, and independent updates</li> </ul>"},{"location":"docker-genai/overview/#inference-server-pattern","title":"Inference Server Pattern","text":"<ul> <li>Dedicated containers running optimized inference servers (like TensorRT, ONNX Runtime, or Triton)</li> <li>Standardized APIs for model access</li> <li>Efficient handling of multiple models and batched requests</li> </ul>"},{"location":"docker-genai/overview/#gpu-sharing-and-allocation","title":"GPU Sharing and Allocation","text":"<ul> <li>Multiple containers sharing GPU resources</li> <li>Strategies for optimal GPU allocation based on model size and demand</li> <li>Tools for monitoring and managing GPU utilization</li> </ul>"},{"location":"docker-genai/overview/#edge-cloud-hybrid-deployment","title":"Edge-Cloud Hybrid Deployment","text":"<ul> <li>Smaller, quantized models in containers at the edge</li> <li>Larger, more powerful models in cloud containers</li> <li>Intelligent routing between edge and cloud based on query complexity</li> </ul>"},{"location":"docker-genai/overview/#docker-vs-alternative-approaches","title":"Docker vs. Alternative Approaches","text":"<p>Understanding how Docker compares to alternative approaches for GenAI deployment:</p>"},{"location":"docker-genai/overview/#docker-vs-bare-metal-deployment","title":"Docker vs. Bare Metal Deployment","text":"Aspect Docker Bare Metal Setup Complexity Low High Resource Efficiency High Highest Deployment Speed Fast Slow Environment Consistency High Low Scaling Flexibility High Low"},{"location":"docker-genai/overview/#docker-vs-traditional-vms","title":"Docker vs. Traditional VMs","text":"Aspect Docker VMs Resource Overhead Low High Startup Time Seconds Minutes Isolation Level Process Hardware Size MB to GB GB to TB Portability Very High Medium"},{"location":"docker-genai/overview/#docker-vs-serverless-ai","title":"Docker vs. Serverless AI","text":"Aspect Docker Serverless Control High Low Cold Start Latency Medium High Cost Predictability High Variable Customization High Limited Management Overhead Medium Low"},{"location":"docker-genai/overview/#dockers-ai-future","title":"Docker's AI Future","text":"<p>Docker continues to evolve its offerings for AI workloads:</p> <ul> <li>AI-optimized Base Images: Specialized base images with pre-installed AI libraries and optimizations</li> <li>Enhanced GPU Support: Improved tooling for GPU allocation and monitoring</li> <li>Integration with ML Platforms: Deeper integration with ML platforms and frameworks</li> <li>Edge AI Support: Expanded support for deploying models to edge devices</li> <li>AI Development Environments: Standardized environments for AI development and experimentation</li> </ul> <p>In the following sections, we'll explore Model Runner and MCP Toolkit in detail, understanding how these specialized Docker components enhance GenAI deployments.</p>"},{"location":"genai-fundamentals/challenges/","title":"Common Challenges in GenAI Deployment","text":"<p>Deploying GenAI applications in production environments presents several significant challenges. Understanding these challenges is essential for designing effective containerized solutions with Docker.</p>"},{"location":"genai-fundamentals/challenges/#resource-management-challenges","title":"Resource Management Challenges","text":""},{"location":"genai-fundamentals/challenges/#high-computational-requirements","title":"High Computational Requirements","text":"<p>GenAI models, particularly large ones, require substantial computational resources:</p> <ul> <li>Memory Consumption: Large models can require 10-100GB+ of RAM</li> <li>GPU Dependencies: Many models need GPU acceleration for reasonable inference speeds</li> <li>Optimization Trade-offs: Balancing model size, performance, and quality</li> </ul>"},{"location":"genai-fundamentals/challenges/#scaling-costs","title":"Scaling Costs","text":"<p>The costs associated with running GenAI workloads can escalate quickly:</p> <ul> <li>GPU Instance Expenses: High costs for specialized hardware</li> <li>Idle Resources: Paying for resources during low-traffic periods</li> <li>Traffic Spikes: Handling sudden increases in demand without overprovisioning</li> </ul>"},{"location":"genai-fundamentals/challenges/#performance-challenges","title":"Performance Challenges","text":""},{"location":"genai-fundamentals/challenges/#latency-management","title":"Latency Management","text":"<p>User experience often depends on fast response times:</p> <ul> <li>Cold Start Problems: Initial loading of large models can take seconds to minutes</li> <li>Inference Speed: Processing time for generating responses</li> <li>End-to-End Latency: Total time from request to response delivery</li> </ul>"},{"location":"genai-fundamentals/challenges/#throughput-limitations","title":"Throughput Limitations","text":"<p>Supporting many concurrent users presents throughput challenges:</p> <ul> <li>Batch Processing: Efficiently handling multiple requests simultaneously</li> <li>Queue Management: Prioritizing and scheduling incoming requests</li> <li>Resource Contention: Managing competition for shared resources</li> </ul>"},{"location":"genai-fundamentals/challenges/#deployment-challenges","title":"Deployment Challenges","text":""},{"location":"genai-fundamentals/challenges/#model-versioning","title":"Model Versioning","text":"<p>Managing multiple versions of models in production:</p> <ul> <li>Consistent Updates: Ensuring smooth transitions between versions</li> <li>Rollback Capabilities: Ability to revert to previous versions when issues arise</li> <li>A/B Testing: Running multiple versions simultaneously for comparison</li> </ul>"},{"location":"genai-fundamentals/challenges/#environment-consistency","title":"Environment Consistency","text":"<p>Maintaining consistent environments across development and production:</p> <ul> <li>Dependency Hell: Managing complex dependencies between models and libraries</li> <li>Hardware Differences: Ensuring models work across different hardware configurations</li> <li>Reproducibility: Creating reproducible environments for debugging and testing</li> </ul>"},{"location":"genai-fundamentals/challenges/#operational-challenges","title":"Operational Challenges","text":""},{"location":"genai-fundamentals/challenges/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>Tracking the health and performance of GenAI systems:</p> <ul> <li>Performance Metrics: Capturing meaningful metrics for GenAI workloads</li> <li>Output Quality: Monitoring the quality and appropriateness of generated content</li> <li>Resource Utilization: Tracking efficient use of computational resources</li> </ul>"},{"location":"genai-fundamentals/challenges/#security-and-compliance","title":"Security and Compliance","text":"<p>Ensuring GenAI deployments meet security and regulatory requirements:</p> <ul> <li>Model Security: Protecting models from unauthorized access or tampering</li> <li>Data Privacy: Handling sensitive data used in model training or inference</li> <li>Compliance Requirements: Meeting industry and regional regulations</li> </ul>"},{"location":"genai-fundamentals/challenges/#integration-challenges","title":"Integration Challenges","text":""},{"location":"genai-fundamentals/challenges/#api-standardization","title":"API Standardization","text":"<p>Creating consistent interfaces for diverse models:</p> <ul> <li>Input/Output Formats: Standardizing data formats across different models</li> <li>Error Handling: Consistent error reporting and recovery</li> <li>Versioning: Managing API changes over time</li> </ul>"},{"location":"genai-fundamentals/challenges/#system-integration","title":"System Integration","text":"<p>Connecting GenAI capabilities with existing systems:</p> <ul> <li>Legacy Integration: Working with older systems not designed for AI workloads</li> <li>Data Flow: Managing data movement between systems</li> <li>State Management: Handling stateful interactions with models</li> </ul>"},{"location":"genai-fundamentals/challenges/#how-docker-addresses-these-challenges","title":"How Docker Addresses These Challenges","text":"<p>Docker provides solutions to many of these challenges:</p>"},{"location":"genai-fundamentals/challenges/#resource-optimization","title":"Resource Optimization","text":"<ul> <li>Container Resource Limits: Fine-grained control over CPU, memory, and GPU allocation</li> <li>Efficient Packaging: Minimizing container size for faster deployment</li> <li>Resource Sharing: Intelligent allocation of resources across containers</li> </ul>"},{"location":"genai-fundamentals/challenges/#performance-enhancement","title":"Performance Enhancement","text":"<ul> <li>Optimized Images: Pre-built containers optimized for AI workloads</li> <li>Caching Strategies: Using Docker layers to speed up deployments</li> <li>Intelligent Scaling: Scaling containers based on demand</li> </ul>"},{"location":"genai-fundamentals/challenges/#deployment-simplification","title":"Deployment Simplification","text":"<ul> <li>Consistent Environments: Same container runs identically across environments</li> <li>Version Control: Tagging and managing different model versions</li> <li>Automated Deployment: CI/CD pipelines for model deployment</li> </ul>"},{"location":"genai-fundamentals/challenges/#operational-improvements","title":"Operational Improvements","text":"<ul> <li>Integrated Monitoring: Built-in tools for observability</li> <li>Resource Isolation: Security benefits from container isolation</li> <li>Orchestration: Kubernetes integration for complex deployments</li> </ul> <p>In the next sections, we'll explore how specialized Docker components like Model Runner and MCP Toolkit specifically address these challenges.</p>"},{"location":"genai-fundamentals/genai-stack/","title":"GenAI Stack","text":"<p>The GenAI stack refers to the complete set of technologies, frameworks, and tools used to develop, deploy, and manage generative AI applications. Understanding this stack is crucial for implementing effective GenAI solutions with Docker.</p>"},{"location":"genai-fundamentals/genai-stack/#components-of-the-genai-stack","title":"Components of the GenAI Stack","text":""},{"location":"genai-fundamentals/genai-stack/#1-foundation-models","title":"1. Foundation Models","text":"<p>At the core of the GenAI stack are the foundation models:</p> <ul> <li>Large Language Models (LLMs): Models like GPT, Llama, Claude, and others that process and generate text</li> <li>Multimodal Models: Models that work with multiple types of data (text, images, audio)</li> <li>Specialized Models: Purpose-built models for specific domains like code, scientific literature, or healthcare</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#2-model-infrastructure","title":"2. Model Infrastructure","text":"<p>The infrastructure layer provides the computational resources and optimization tools:</p> <ul> <li>Hardware Acceleration: GPUs, TPUs, and specialized AI chips</li> <li>Distributed Computing: Systems for training and running models across multiple nodes</li> <li>Quantization: Techniques to reduce model size and increase inference speed</li> <li>Model Optimization: Methods to improve efficiency without sacrificing quality</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#3-orchestration-deployment","title":"3. Orchestration &amp; Deployment","text":"<p>This layer manages how models are deployed and scaled:</p> <ul> <li>Containerization: Docker containers for packaging models and dependencies</li> <li>Orchestration: Kubernetes and similar tools for managing deployed models</li> <li>API Gateways: Managing access to model endpoints</li> <li>Load Balancing: Distributing requests across model instances</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#4-serving-infrastructure","title":"4. Serving Infrastructure","text":"<p>The serving layer handles how models respond to requests:</p> <ul> <li>Inference Servers: Specialized servers optimized for model inference</li> <li>Caching Systems: Storing common outputs to improve response times</li> <li>Batch Processing: Handling large volumes of non-real-time requests</li> <li>Streaming: Managing continuous output generation</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#5-application-integration","title":"5. Application Integration","text":"<p>This layer connects GenAI capabilities with business applications:</p> <ul> <li>APIs &amp; SDKs: Interfaces for developers to access models</li> <li>Webhooks: Event-driven integration patterns</li> <li>Plugins: Extending applications with GenAI capabilities</li> <li>Embeddings: Integration of vector databases for semantic search</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#6-development-tools","title":"6. Development Tools","text":"<p>Tools that help developers work with GenAI:</p> <ul> <li>Prompt Engineering Tools: Designing and testing prompts</li> <li>RAG Frameworks: Retrieval-Augmented Generation implementation tools</li> <li>Fine-tuning Platforms: Customizing models for specific use cases</li> <li>Evaluation Frameworks: Testing model outputs and performance</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#7-monitoring-observability","title":"7. Monitoring &amp; Observability","text":"<p>Systems to track the operation and performance of GenAI applications:</p> <ul> <li>Performance Monitoring: Tracking latency, throughput, and resource usage</li> <li>Output Quality: Assessing the quality and appropriateness of generated content</li> <li>Usage Analytics: Understanding how models are being used</li> <li>Drift Detection: Identifying when model performance degrades over time</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#dockers-role-in-the-genai-stack","title":"Docker's Role in the GenAI Stack","text":"<p>Docker provides several key components that integrate with the GenAI stack:</p>"},{"location":"genai-fundamentals/genai-stack/#model-runner","title":"Model Runner","text":"<p>Model Runner is Docker's specialized runtime for AI models, offering:</p> <ul> <li>Optimized Containers: Pre-configured for AI workloads</li> <li>Model Serving: Standardized APIs for model inference</li> <li>Performance Tuning: Tools to optimize model performance</li> <li>Monitoring Integration: Built-in metrics and logging</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#mcp-toolkit","title":"MCP Toolkit","text":"<p>The Model Container Platform (MCP) Toolkit provides:</p> <ul> <li>Model Management: Tools for managing model lifecycles</li> <li>Deployment Automation: Streamlined deployment workflows</li> <li>Scaling Policies: Intelligent scaling based on demand</li> <li>Resource Optimization: Efficient allocation of computing resources</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#docker-extensions-for-ai","title":"Docker Extensions for AI","text":"<p>Docker offers extensions specifically for AI workflows:</p> <ul> <li>Development Environments: Pre-configured environments for AI development</li> <li>CI/CD Integration: Automated testing and deployment of models</li> <li>Registry Features: Special handling for large model artifacts</li> <li>Security Scanning: Vulnerability assessment for AI containers</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#common-genai-stack-patterns","title":"Common GenAI Stack Patterns","text":"<p>Several common patterns have emerged for containerized GenAI deployments:</p>"},{"location":"genai-fundamentals/genai-stack/#microservices-architecture","title":"Microservices Architecture","text":"<p>Breaking GenAI applications into specialized services:</p> <ul> <li>Model Servers: Containers dedicated to serving specific models</li> <li>Preprocessing Services: Handling input normalization</li> <li>Orchestration Services: Managing complex multi-model workflows</li> <li>Caching Layers: Improving response times for common queries</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#hybrid-deployment","title":"Hybrid Deployment","text":"<p>Distributing GenAI workloads across environments:</p> <ul> <li>Cloud-Edge Split: Running different components in cloud vs. edge devices</li> <li>Multi-Cloud Distribution: Spreading workloads across cloud providers</li> <li>On-Premise Integration: Connecting on-premise systems with cloud AI services</li> </ul>"},{"location":"genai-fundamentals/genai-stack/#scalable-inference","title":"Scalable Inference","text":"<p>Patterns for handling varying loads:</p> <ul> <li>Auto-scaling Pools: Dynamically adjusting the number of inference containers</li> <li>Priority Queuing: Managing requests based on importance</li> <li>Tiered Service Levels: Offering different performance guarantees</li> </ul> <p>In the next section, we'll explore the common challenges faced when deploying GenAI applications and how Docker helps address them.</p>"},{"location":"genai-fundamentals/key-concepts/","title":"Key Concepts in GenAI","text":"<p>This page covers the essential concepts in Generative AI that form the foundation of the GenAI Docker Workshop.</p>"},{"location":"genai-fundamentals/key-concepts/#foundation-models","title":"Foundation Models","text":"<p>Foundation models are large-scale AI models trained on vast datasets that can be adapted for a wide range of applications. They serve as the basis for many GenAI applications.</p>"},{"location":"genai-fundamentals/key-concepts/#types-of-foundation-models","title":"Types of Foundation Models","text":"<ul> <li>Large Language Models (LLMs): Models like GPT-4, Claude, Llama, and PaLM that process and generate text.</li> <li>Multimodal Models: Models like DALL-E, Stable Diffusion, and Midjourney that can work with multiple types of data (e.g., text and images).</li> <li>Domain-Specific Models: Models fine-tuned for specific domains like medicine, law, or code generation.</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Scale: Typically have billions or trillions of parameters</li> <li>Self-supervised Learning: Trained on unlabeled data</li> <li>Transfer Learning: Can transfer knowledge to new tasks</li> <li>Few-shot Learning: Can perform tasks with few examples</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#transformer-architecture","title":"Transformer Architecture","text":"<p>The transformer architecture is the backbone of most modern GenAI models, providing significant advantages over previous approaches.</p>"},{"location":"genai-fundamentals/key-concepts/#components","title":"Components","text":"<ul> <li>Self-Attention Mechanism: Allows the model to weigh the importance of different parts of the input</li> <li>Multi-Head Attention: Enables the model to attend to information from different representation subspaces</li> <li>Positional Encoding: Provides information about the position of tokens in the sequence</li> <li>Feed-Forward Networks: Process the attention output</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#advantages","title":"Advantages","text":"<ul> <li>Parallelization: Can process all tokens simultaneously</li> <li>Long-Range Dependencies: Better handling of long-distance relationships in data</li> <li>Scalability: Architecture scales well with more data and parameters</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#inference-optimization","title":"Inference Optimization","text":"<p>Inference optimization techniques make GenAI models faster and more efficient for deployment.</p>"},{"location":"genai-fundamentals/key-concepts/#quantization","title":"Quantization","text":"<p>Reducing the precision of model weights:</p> <ul> <li>FP16: 16-bit floating-point precision</li> <li>INT8: 8-bit integer precision</li> <li>INT4: 4-bit integer precision</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#pruning","title":"Pruning","text":"<p>Removing unnecessary connections or weights from the model:</p> <ul> <li>Structured Pruning: Removing entire neurons or layers</li> <li>Unstructured Pruning: Removing individual weights</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#distillation","title":"Distillation","text":"<p>Creating smaller models that learn from larger ones:</p> <ul> <li>Knowledge Distillation: Training a smaller \"student\" model to mimic a larger \"teacher\" model</li> <li>Distilled Models: Examples include DistilBERT, TinyLlama</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#prompt-engineering","title":"Prompt Engineering","text":"<p>Prompt engineering is the practice of designing inputs to GenAI models to get desired outputs.</p>"},{"location":"genai-fundamentals/key-concepts/#techniques","title":"Techniques","text":"<ul> <li>Zero-shot Prompting: Asking the model to perform a task without examples</li> <li>Few-shot Prompting: Providing a few examples in the prompt</li> <li>Chain-of-Thought: Guiding the model through a reasoning process</li> <li>System Prompts: Setting the context and behavior of the model</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#applications","title":"Applications","text":"<ul> <li>Task Definition: Clearly defining what the model should do</li> <li>Persona Design: Establishing a specific role or voice</li> <li>Output Formatting: Specifying how outputs should be structured</li> <li>Constraint Setting: Defining limitations or requirements</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":"<p>RAG combines retrieval mechanisms with generative models to produce more accurate and grounded outputs.</p>"},{"location":"genai-fundamentals/key-concepts/#components_1","title":"Components","text":"<ul> <li>Vector Database: Stores embeddings of documents or knowledge</li> <li>Retriever: Finds relevant information from the knowledge base</li> <li>Generator: Creates responses based on the retrieved information</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#benefits","title":"Benefits","text":"<ul> <li>Factual Accuracy: Reduces hallucinations by grounding in retrieved facts</li> <li>Up-to-date Information: Can access the latest information beyond training data</li> <li>Domain Adaptation: Easier adaptation to specific domains</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#fine-tuning","title":"Fine-tuning","text":"<p>Fine-tuning is the process of adapting a pre-trained model to specific tasks or domains.</p>"},{"location":"genai-fundamentals/key-concepts/#methods","title":"Methods","text":"<ul> <li>Full Fine-tuning: Updating all parameters of the model</li> <li>Parameter-Efficient Fine-tuning (PEFT): Updating a small subset of parameters</li> <li>LoRA (Low-Rank Adaptation): Adding low-rank matrices to existing weights</li> <li>QLoRA: Quantized version of LoRA for efficiency</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#applications_1","title":"Applications","text":"<ul> <li>Domain Adaptation: Adapting to specific industries or fields</li> <li>Task Specialization: Optimizing for particular tasks</li> <li>Style Alignment: Adjusting the style or tone of outputs</li> <li>Instruction Tuning: Training models to follow instructions</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Evaluation metrics help assess the performance and quality of GenAI models.</p>"},{"location":"genai-fundamentals/key-concepts/#quality-metrics","title":"Quality Metrics","text":"<ul> <li>BLEU, ROUGE: Text similarity metrics</li> <li>Human Evaluation: Manual assessment of outputs</li> <li>LLM-as-Judge: Using other LLMs to evaluate outputs</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Latency: Response time</li> <li>Throughput: Requests processed per second</li> <li>Token Generation Speed: Tokens generated per second</li> <li>Resource Utilization: CPU, memory, and GPU usage</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#deployment-architectures","title":"Deployment Architectures","text":"<p>Various architectures are used to deploy GenAI models in production environments.</p>"},{"location":"genai-fundamentals/key-concepts/#serving-patterns","title":"Serving Patterns","text":"<ul> <li>Direct Inference: Sending requests directly to the model</li> <li>Batch Processing: Processing multiple requests in batches</li> <li>Streaming: Generating and returning responses incrementally</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#scaling-approaches","title":"Scaling Approaches","text":"<ul> <li>Horizontal Scaling: Adding more model instances</li> <li>Vertical Scaling: Using more powerful hardware</li> <li>Hybrid Approaches: Combining different scaling strategies</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#deployment-options","title":"Deployment Options","text":"<ul> <li>Cloud Deployment: Using cloud providers' infrastructure</li> <li>On-premises Deployment: Deploying within an organization's infrastructure</li> <li>Edge Deployment: Running models on edge devices</li> <li>Hybrid Deployment: Combining cloud and on-premises resources</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#ethical-considerations","title":"Ethical Considerations","text":"<p>Ethical considerations are essential when deploying GenAI applications.</p>"},{"location":"genai-fundamentals/key-concepts/#key-concerns","title":"Key Concerns","text":"<ul> <li>Bias: Models may perpetuate or amplify societal biases</li> <li>Privacy: Handling sensitive data and user information</li> <li>Transparency: Understanding how models make decisions</li> <li>Safety: Preventing harmful or inappropriate outputs</li> </ul>"},{"location":"genai-fundamentals/key-concepts/#mitigation-strategies","title":"Mitigation Strategies","text":"<ul> <li>Red-teaming: Testing models for harmful capabilities</li> <li>RLHF (Reinforcement Learning from Human Feedback): Aligning models with human values</li> <li>Content Filtering: Preventing inappropriate outputs</li> <li>Usage Policies: Clear guidelines for acceptable use</li> </ul> <p>Understanding these key concepts provides the foundation for effectively using Docker to deploy and manage GenAI applications, which we'll explore in the practical sections of this workshop.</p>"},{"location":"genai-fundamentals/overview/","title":"GenAI Overview","text":""},{"location":"genai-fundamentals/overview/#what-is-generative-ai","title":"What is Generative AI?","text":"<p>Generative AI (GenAI) refers to a category of artificial intelligence systems that can create new content including text, images, audio, video, code, and more. Unlike traditional AI systems that are designed to recognize patterns or make predictions based on existing data, generative AI can produce entirely new outputs that didn't exist before.</p> <p>GenAI models learn from vast amounts of training data to understand patterns, relationships, and structures, then use this knowledge to generate new content that resembles what they were trained on but with novel combinations and variations.</p>"},{"location":"genai-fundamentals/overview/#core-technologies-behind-genai","title":"Core Technologies Behind GenAI","text":""},{"location":"genai-fundamentals/overview/#transformers","title":"Transformers","text":"<p>Transformer architectures have revolutionized natural language processing and are the foundation of most modern GenAI models. Key features include:</p> <ul> <li>Self-attention mechanisms that allow the model to weigh the importance of different parts of the input</li> <li>Parallelization that enables efficient training on large datasets</li> <li>Contextual understanding that helps capture relationships between words or tokens</li> </ul>"},{"location":"genai-fundamentals/overview/#foundation-models","title":"Foundation Models","text":"<p>Foundation models are large-scale AI models trained on vast and diverse datasets that can be adapted for a wide range of applications:</p> <ul> <li>Large Language Models (LLMs): Models like GPT-4, Claude, Llama, and PaLM that process and generate human language</li> <li>Multimodal Models: Systems like DALL-E, Midjourney, and Stable Diffusion that work across different types of data (text, images, etc.)</li> <li>Code Generation Models: Specialized models like GitHub Copilot that can generate and understand programming code</li> </ul>"},{"location":"genai-fundamentals/overview/#applications-of-genai","title":"Applications of GenAI","text":"<p>GenAI is being applied across numerous domains:</p> <ul> <li>Content Creation: Generating articles, marketing copy, creative writing, and scripts</li> <li>Software Development: Code generation, debugging, and documentation</li> <li>Design: Creating images, graphics, UI elements, and product designs</li> <li>Customer Service: Powering intelligent chatbots and virtual assistants</li> <li>Healthcare: Generating medical reports and assisting with diagnostics</li> <li>Finance: Creating reports, analyzing trends, and generating risk assessments</li> <li>Education: Creating personalized learning materials and assessments</li> </ul>"},{"location":"genai-fundamentals/overview/#genai-deployment-challenges","title":"GenAI Deployment Challenges","text":"<p>Deploying GenAI applications presents several key challenges:</p> <ol> <li>Computational Requirements: Many GenAI models require significant computing resources</li> <li>Latency: Real-time applications need fast response times</li> <li>Scalability: Handling varying levels of user demand</li> <li>Model Updates: Deploying new versions without disrupting service</li> <li>Integration: Connecting GenAI capabilities with existing systems</li> <li>Monitoring: Tracking model performance and behavior</li> <li>Cost Management: Optimizing resource usage to control expenses</li> </ol>"},{"location":"genai-fundamentals/overview/#dockers-role-in-genai","title":"Docker's Role in GenAI","text":"<p>Docker addresses many GenAI deployment challenges through containerization:</p> <ul> <li>Consistent Environments: Ensuring models run the same way in development and production</li> <li>Resource Isolation: Allocating appropriate resources to each model</li> <li>Scalable Deployments: Scaling instances based on demand</li> <li>Version Control: Managing different versions of models and dependencies</li> <li>Orchestration: Using Kubernetes or Docker Swarm for complex deployments</li> <li>Optimized Packaging: Creating lightweight, efficient containers for AI workloads</li> </ul>"},{"location":"genai-fundamentals/overview/#the-evolution-of-genai-and-docker","title":"The Evolution of GenAI and Docker","text":"<p>The integration of Docker with GenAI represents a significant advancement in how AI applications are developed and deployed:</p> <ul> <li>Simplified Workflows: From research to production deployment</li> <li>Democratized Access: Making powerful models accessible to more developers</li> <li>Enterprise Adoption: Enabling organizations to implement GenAI solutions safely</li> <li>Edge Deployment: Running models on edge devices with limited resources</li> <li>Hybrid Scenarios: Distributing model components across cloud and edge</li> </ul> <p>In the following sections, we'll explore the GenAI stack in more detail and learn how Docker's specialized tools can help overcome common deployment challenges.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This guide will walk you through setting up your environment for the GenAI Docker Workshop.</p>"},{"location":"getting-started/installation/#1-docker-installation","title":"1. Docker Installation","text":"<p>Docker is the primary platform we'll use throughout this workshop. Follow these steps to install Docker based on your operating system.</p>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<ol> <li>System Requirements:</li> <li>Windows 10 64-bit: Pro, Enterprise, or Education (Build 17763 or later)</li> <li> <p>Enable Hyper-V and Containers Windows features</p> </li> <li> <p>Installation Steps:</p> </li> <li>Download Docker Desktop from Docker's website</li> <li>Run the installer and follow the prompts</li> <li>After installation, Docker Desktop will start automatically</li> </ol>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<ol> <li>System Requirements:</li> <li>macOS must be version 10.15 or newer</li> <li> <p>For Apple Silicon Macs, ensure you have Rosetta 2 installed</p> </li> <li> <p>Installation Steps:</p> </li> <li>Download Docker Desktop from Docker's website</li> <li>Drag the Docker icon to the Applications folder</li> <li>Start Docker from the Applications folder</li> </ol>"},{"location":"getting-started/installation/#linux","title":"Linux","text":"<p>For Linux, you'll install Docker Engine directly:</p> <pre><code># Update package index\nsudo apt-get update\n\n# Install prerequisites\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n\n# Add Docker's official GPG key\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n\n# Set up the stable repository\necho \\\n  \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# Install Docker Engine\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\n# Add your user to the docker group to run docker without sudo\nsudo usermod -aG docker $USER\n</code></pre> <p>Note: You may need to log out and back in for the group changes to take effect.</p>"},{"location":"getting-started/installation/#2-model-runner-installation","title":"2. Model Runner Installation","text":"<p>Model Runner is a specialized Docker component for running GenAI models efficiently. Here's how to install it:</p> <pre><code># Pull the Model Runner image\ndocker pull docker/genai-model-runner:latest\n\n# Verify the installation\ndocker run --rm docker/genai-model-runner:latest --version\n</code></pre>"},{"location":"getting-started/installation/#3-mcp-toolkit-installation","title":"3. MCP Toolkit Installation","text":"<p>The MCP (Model Container Platform) Toolkit helps manage containerized AI models:</p> <pre><code># Pull the MCP Toolkit image\ndocker pull docker/mcp-toolkit:latest\n\n# Set up MCP CLI\ndocker run --rm -v /usr/local/bin:/target docker/mcp-toolkit:latest install\n</code></pre>"},{"location":"getting-started/installation/#4-workshop-environment-setup","title":"4. Workshop Environment Setup","text":"<p>Set up the workshop environment by cloning the repository and installing dependencies:</p> <pre><code># Clone the workshop repository\ngit clone https://github.com/collabnix/genai-docker-workshop.git\ncd genai-docker-workshop\n\n# Clone lab repositories\ngit clone https://github.com/ajeetraina/genai-model-runner-metrics.git\ngit clone https://github.com/ajeetraina/catalog-service-demo.git\n</code></pre>"},{"location":"getting-started/installation/#5-verify-installation","title":"5. Verify Installation","text":"<p>Verify that everything is installed correctly:</p> <pre><code># Check Docker\ndocker --version\n\n# Test Docker with a simple container\ndocker run hello-world\n\n# Check Model Runner\ndocker run --rm docker/genai-model-runner:latest --version\n\n# Check MCP Toolkit\nmcp --version  # If you installed the CLI\n</code></pre> <p>If all commands run without errors, your environment is set up correctly!</p>"},{"location":"getting-started/installation/#6-docker-compose-installation-if-not-included","title":"6. Docker Compose Installation (If Not Included)","text":"<p>Docker Compose should be included with Docker Desktop. If you're on Linux and need to install it separately:</p> <pre><code># Install Docker Compose\nsudo curl -L \"https://github.com/docker/compose/releases/download/v2.18.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\nsudo chmod +x /usr/local/bin/docker-compose\n\n# Verify the installation\ndocker-compose --version\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation:</p> <ol> <li>Docker fails to start:</li> <li>On Windows, ensure Hyper-V is enabled</li> <li>On macOS, check System Preferences for any security blocks</li> <li> <p>On Linux, ensure the Docker daemon is running: <code>sudo systemctl start docker</code></p> </li> <li> <p>Permission issues:</p> </li> <li> <p>On Linux, ensure your user is added to the docker group and you've logged out and back in</p> </li> <li> <p>Network issues:</p> </li> <li>Check your firewall settings</li> <li>Ensure Docker can access the internet for pulling images</li> </ol> <p>For any other issues, refer to the Docker documentation or reach out to the workshop facilitators.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have installed all the necessary components, you're ready to dive into the GenAI Fundamentals!</p>"},{"location":"getting-started/introduction/","title":"Introduction to GenAI with Docker","text":""},{"location":"getting-started/introduction/#what-is-genai","title":"What is GenAI?","text":"<p>Generative AI (GenAI) refers to artificial intelligence systems that can generate new content, including text, images, audio, and more. These systems are trained on vast amounts of data and learn patterns that allow them to create outputs that resemble human-created content.</p> <p>Key aspects of GenAI include:</p> <ul> <li>Content generation: Creating new text, images, code, or other media</li> <li>Pattern recognition: Learning from existing data to generate relevant outputs</li> <li>Contextual understanding: Responding appropriately to prompts or instructions</li> <li>Creative applications: Assisting with writing, design, programming, and more</li> </ul>"},{"location":"getting-started/introduction/#why-docker-for-genai","title":"Why Docker for GenAI?","text":"<p>Docker provides an ideal platform for deploying and managing GenAI applications for several reasons:</p> <ol> <li>Consistency: Docker ensures that your AI models run the same way across all environments.</li> <li>Isolation: Containerized AI models are isolated from other applications, preventing conflicts.</li> <li>Scalability: Docker makes it easy to scale your GenAI applications up or down based on demand.</li> <li>Dependency Management: Docker containers include all necessary dependencies, eliminating \"it works on my machine\" problems.</li> <li>Resource Efficiency: Containers are lightweight compared to traditional VMs, making better use of your infrastructure.</li> <li>Orchestration: With Kubernetes or Docker Swarm, you can orchestrate complex GenAI deployments.</li> </ol>"},{"location":"getting-started/introduction/#docker-components-for-genai","title":"Docker Components for GenAI","text":"<p>Docker provides several tools specialized for GenAI workloads:</p> <ul> <li>Model Runner: An optimized runtime for deploying and serving AI models</li> <li>MCP Toolkit: Docker tools for managing machine learning container platforms</li> <li>Docker Hub for AI Models: Repository of pre-built AI model containers</li> </ul>"},{"location":"getting-started/introduction/#workshop-overview","title":"Workshop Overview","text":"<p>This workshop will guide you through:</p> <ol> <li>Setting up your environment for GenAI development with Docker</li> <li>Understanding the architecture of containerized GenAI applications</li> <li>Working with Model Runner for efficient model deployment</li> <li>Implementing common GenAI use cases with Docker</li> <li>Deploying and monitoring GenAI applications in production</li> </ol> <p>Let's get started by ensuring you have all the prerequisites in place!</p>"},{"location":"getting-started/prerequisites/","title":"Prerequisites","text":"<p>Before starting the GenAI Docker Workshop, please ensure you have the following prerequisites set up:</p>"},{"location":"getting-started/prerequisites/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CPU: 4+ cores recommended</li> <li>RAM: Minimum 8GB, 16GB+ recommended for running models locally</li> <li>Disk Space: At least 20GB of free space</li> <li>GPU: Optional but recommended for model training and faster inference (NVIDIA GPU with CUDA support)</li> </ul>"},{"location":"getting-started/prerequisites/#software-requirements","title":"Software Requirements","text":""},{"location":"getting-started/prerequisites/#required-software","title":"Required Software","text":"<ol> <li>Docker</li> <li>Docker Desktop for Windows or Mac</li> <li>Docker Engine for Linux</li> <li>Docker Compose (included in Docker Desktop)</li> <li> <p>Verify installation with:      <pre><code>docker --version\ndocker-compose --version\n</code></pre></p> </li> <li> <p>Git</p> </li> <li>Download and install from git-scm.com</li> <li> <p>Verify installation with:      <pre><code>git --version\n</code></pre></p> </li> <li> <p>Python 3.8+</p> </li> <li>Download from python.org or use your system's package manager</li> <li> <p>Verify installation with:      <pre><code>python --version\n</code></pre></p> </li> <li> <p>Text Editor or IDE</p> </li> <li>Visual Studio Code, PyCharm, or any preferred code editor</li> </ol>"},{"location":"getting-started/prerequisites/#optional-but-recommended","title":"Optional but Recommended","text":"<ol> <li>NVIDIA Container Toolkit (for GPU support)</li> <li>Install only if you have an NVIDIA GPU</li> <li> <p>Follow installation instructions</p> </li> <li> <p>kubectl</p> </li> <li>For Kubernetes-based deployments</li> <li>Install from kubernetes.io</li> </ol>"},{"location":"getting-started/prerequisites/#knowledge-prerequisites","title":"Knowledge Prerequisites","text":"<ul> <li>Basic understanding of Docker concepts (containers, images, volumes)</li> <li>Familiarity with command-line operations</li> <li>Basic understanding of Python programming</li> <li>Basic knowledge of AI/ML concepts (helpful but not required)</li> </ul>"},{"location":"getting-started/prerequisites/#workshop-materials","title":"Workshop Materials","text":"<p>Please clone the following repositories before starting the workshop:</p> <pre><code># Main workshop repository\ngit clone https://github.com/collabnix/genai-docker-workshop.git\n\n# Lab 1 - Model Runner Metrics\ngit clone https://github.com/ajeetraina/genai-model-runner-metrics.git\n\n# Lab 2 - Product Catalog Demo\ngit clone https://github.com/ajeetraina/catalog-service-demo.git\n</code></pre>"},{"location":"getting-started/prerequisites/#network-requirements","title":"Network Requirements","text":"<ul> <li>Stable internet connection for downloading Docker images and dependencies</li> <li>If working in a corporate environment, ensure you have access to:</li> <li>Docker Hub</li> <li>GitHub</li> <li>PyPI (Python Package Index)</li> </ul>"},{"location":"getting-started/prerequisites/#environment-setup-verification","title":"Environment Setup Verification","text":"<p>To verify that your environment is properly set up, run the following test container:</p> <pre><code>docker run --rm hello-world\n</code></pre> <p>If you see a \"Hello from Docker!\" message, your Docker installation is working correctly.</p> <p>If you're ready to proceed, head over to the Installation section to set up your development environment.</p>"},{"location":"labs/lab1-model-runner-metrics/","title":"Lab 1: Model Runner Metrics","text":"<p>In this lab, we'll explore how to use Docker's Model Runner to deploy a generative AI model and collect performance metrics. We'll use the genai-model-runner-metrics project to visualize and analyze these metrics.</p>"},{"location":"labs/lab1-model-runner-metrics/#objectives","title":"Objectives","text":"<p>By the end of this lab, you will be able to:</p> <ol> <li>Deploy a language model using Docker's Model Runner</li> <li>Configure and collect performance metrics</li> <li>Visualize metrics using Prometheus and Grafana</li> <li>Analyze performance under different load conditions</li> <li>Optimize model configuration based on metrics</li> </ol>"},{"location":"labs/lab1-model-runner-metrics/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following set up:</p> <ul> <li>Docker Desktop installed and running</li> <li>Git installed</li> <li>At least 8GB of RAM available</li> <li>Approximately 20GB of free disk space</li> <li>Basic knowledge of Docker and Docker Compose</li> <li>The lab repository cloned:   <pre><code>git clone https://github.com/ajeetraina/genai-model-runner-metrics.git\ncd genai-model-runner-metrics\n</code></pre></li> </ul>"},{"location":"labs/lab1-model-runner-metrics/#lab-instructions","title":"Lab Instructions","text":"<p>Detailed instructions for this lab can be found in the genai-model-runner-metrics repository.</p>"},{"location":"labs/lab1-model-runner-metrics/#key-steps","title":"Key Steps","text":"<ol> <li>Clone the lab repository</li> <li>Start the services using Docker Compose</li> <li>Access the model through the provided API</li> <li>Explore metrics in Prometheus and Grafana</li> <li>Run load tests to analyze performance</li> <li>Optimize model configuration</li> </ol>"},{"location":"labs/lab1-model-runner-metrics/#resources","title":"Resources","text":"<ul> <li>Model Runner Documentation</li> <li>Prometheus Documentation</li> <li>Grafana Documentation</li> </ul>"},{"location":"labs/lab1-model-runner-metrics/#next-steps","title":"Next Steps","text":"<p>After completing this lab, proceed to Lab 2: Product Catalog Demo to explore a more complex GenAI application.</p>"},{"location":"labs/lab2-product-catalog/","title":"Lab 2: Product Catalog Demo","text":"<p>In this lab, we'll implement a practical GenAI application: a product catalog service enhanced with AI capabilities. We'll use the catalog-service-demo project to build, deploy, and interact with this application.</p>"},{"location":"labs/lab2-product-catalog/#objectives","title":"Objectives","text":"<p>By the end of this lab, you will be able to:</p> <ol> <li>Deploy a microservices architecture with GenAI components</li> <li>Integrate a language model into a product catalog application</li> <li>Use Docker to manage the complete application stack</li> <li>Test AI-enhanced product search and recommendations</li> <li>Understand the patterns for integrating GenAI into business applications</li> </ol>"},{"location":"labs/lab2-product-catalog/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following set up:</p> <ul> <li>Docker Desktop installed and running</li> <li>Git installed</li> <li>At least 8GB of RAM available</li> <li>Basic knowledge of Docker, Docker Compose, and REST APIs</li> <li>The lab repository cloned:   <pre><code>git clone https://github.com/ajeetraina/catalog-service-demo.git\ncd catalog-service-demo\n</code></pre></li> </ul>"},{"location":"labs/lab2-product-catalog/#lab-instructions","title":"Lab Instructions","text":"<p>Detailed instructions for this lab can be found in the catalog-service-demo repository.</p>"},{"location":"labs/lab2-product-catalog/#key-steps","title":"Key Steps","text":"<ol> <li>Clone the lab repository</li> <li>Build and start the application using Docker Compose</li> <li>Explore the product catalog through the web interface</li> <li>Test the AI-enhanced search capabilities</li> <li>Examine the recommendation engine</li> <li>Analyze the microservices architecture</li> </ol>"},{"location":"labs/lab2-product-catalog/#application-architecture","title":"Application Architecture","text":"<p>The application consists of several microservices:</p> <ul> <li>Catalog Service: Core service managing product data</li> <li>Search Service: AI-enhanced search functionality</li> <li>Recommendation Engine: AI-powered product recommendations</li> <li>Frontend: Web interface for interacting with the catalog</li> <li>Database: Storage for product data</li> <li>Vector Store: For semantic search capabilities</li> <li>Model Service: Containerized language model for AI features</li> </ul>"},{"location":"labs/lab2-product-catalog/#resources","title":"Resources","text":"<ul> <li>Docker Compose Documentation</li> <li>Microservices Architecture Patterns</li> <li>Vector Database Concepts</li> </ul>"},{"location":"labs/lab2-product-catalog/#next-steps","title":"Next Steps","text":"<p>After completing this lab, explore the Resources section for additional learning materials and community resources.</p>"},{"location":"resources/community/","title":"Community","text":"<p>Join the Collabnix community to connect with other Docker and GenAI enthusiasts, share knowledge, and collaborate on projects.</p>"},{"location":"resources/community/#collabnix-community","title":"Collabnix Community","text":"<p>Collabnix is a community-driven platform focused on Docker, Kubernetes, Cloud Native technologies, and now GenAI. The community offers:</p> <ul> <li>Technical blogs and tutorials</li> <li>Hands-on workshops and labs</li> <li>Open-source project collaboration</li> <li>Knowledge sharing and networking opportunities</li> </ul>"},{"location":"resources/community/#how-to-join","title":"How to Join","text":""},{"location":"resources/community/#community-platforms","title":"Community Platforms","text":"<ul> <li>Slack: Join our Collabnix Slack Workspace for real-time discussions</li> <li>GitHub: Contribute to our repositories</li> <li>LinkedIn: Follow our LinkedIn Page</li> <li>Twitter: Follow @collabnix for updates and announcements</li> </ul>"},{"location":"resources/community/#meetups-and-events","title":"Meetups and Events","text":"<ul> <li>Collabnix regularly organizes virtual and in-person meetups</li> <li>Docker Community All-Hands meetings</li> <li>DockerCon and KubeCon participation</li> <li>Hands-on workshops like this GenAI Docker Workshop</li> </ul>"},{"location":"resources/community/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"resources/community/#content-contribution","title":"Content Contribution","text":"<ul> <li>Blog Posts: Write technical articles for the Collabnix blog</li> <li>Workshop Materials: Help improve existing workshops or create new ones</li> <li>Documentation: Improve documentation and guides</li> </ul>"},{"location":"resources/community/#code-contribution","title":"Code Contribution","text":"<ul> <li>Open Source Projects: Contribute to Collabnix GitHub repositories</li> <li>Bug Fixes: Help fix issues in existing projects</li> <li>Feature Development: Implement new features for community tools</li> </ul>"},{"location":"resources/community/#community-support","title":"Community Support","text":"<ul> <li>Answer Questions: Help others on Slack and GitHub discussions</li> <li>Mentoring: Guide newcomers to Docker and GenAI technologies</li> <li>Event Organization: Help organize community events and meetups</li> </ul>"},{"location":"resources/community/#community-guidelines","title":"Community Guidelines","text":"<ol> <li>Be Respectful: Treat all community members with respect and kindness</li> <li>Share Knowledge: Be generous with your knowledge and expertise</li> <li>Give Credit: Acknowledge others' contributions and ideas</li> <li>Stay Relevant: Keep discussions on-topic and technical</li> <li>No Promotions: Avoid self-promotion or spam</li> </ol>"},{"location":"resources/community/#featured-community-projects","title":"Featured Community Projects","text":""},{"location":"resources/community/#docker-projects","title":"Docker Projects","text":"<ul> <li>Docker Labs - Docker tutorials and labs</li> <li>Docker Community - Community resources and guides</li> <li>DockerBangalore - Docker Bangalore community</li> </ul>"},{"location":"resources/community/#genai-projects","title":"GenAI Projects","text":"<ul> <li>GenAI Docker Workshop - This workshop on GenAI with Docker</li> <li>GenAI Model Runner Metrics - Tools for monitoring GenAI models</li> <li>Catalog Service Demo - Demo of GenAI-enhanced product catalog</li> </ul>"},{"location":"resources/community/#community-success-stories","title":"Community Success Stories","text":"<ul> <li>Helped thousands of engineers learn Docker and Kubernetes</li> <li>Created one of the largest repositories of Docker tutorials</li> <li>Built a global community spanning 150+ countries</li> <li>Organized dozens of successful workshops and meetups</li> </ul>"},{"location":"resources/community/#contact-us","title":"Contact Us","text":"<p>If you have questions or suggestions, feel free to reach out:</p> <ul> <li>Email: community@collabnix.com</li> <li>Twitter: @collabnix</li> <li>GitHub: File an issue</li> </ul> <p>Join us in building a vibrant community around Docker, GenAI, and cloud-native technologies!</p>"},{"location":"resources/faq/","title":"Frequently Asked Questions","text":"<p>This page addresses common questions about GenAI with Docker, the workshop, and related technologies.</p>"},{"location":"resources/faq/#general-questions","title":"General Questions","text":""},{"location":"resources/faq/#what-is-genai","title":"What is GenAI?","text":"<p>Generative AI (GenAI) refers to artificial intelligence systems that can create new content, including text, images, audio, and more. These systems are trained on vast amounts of data and learn patterns that allow them to generate outputs resembling human-created content.</p>"},{"location":"resources/faq/#why-use-docker-for-genai-applications","title":"Why use Docker for GenAI applications?","text":"<p>Docker provides several benefits for GenAI applications: - Consistency: Ensuring models run the same way across all environments - Dependency Management: Packaging all dependencies together - Resource Isolation: Allocating appropriate resources to each model - Scalability: Easily scaling instances based on demand - Reproducibility: Creating reproducible environments for development and production</p>"},{"location":"resources/faq/#do-i-need-a-gpu-to-run-genai-models-with-docker","title":"Do I need a GPU to run GenAI models with Docker?","text":"<p>While many GenAI models benefit significantly from GPU acceleration, it's not always required: - Development &amp; Testing: You can develop and test with CPU-only configurations - Small Models: Smaller, optimized models can run reasonably well on CPUs - Quantized Models: INT8 or FP16 quantized models require less computational power - Cloud GPUs: You can use cloud-based GPU resources for production deployment</p>"},{"location":"resources/faq/#workshop-specific-questions","title":"Workshop-Specific Questions","text":""},{"location":"resources/faq/#what-knowledge-do-i-need-before-taking-this-workshop","title":"What knowledge do I need before taking this workshop?","text":"<p>This workshop assumes: - Basic understanding of Docker concepts (containers, images, volumes) - Familiarity with command-line operations - Basic understanding of Python programming - Basic knowledge of AI/ML concepts (helpful but not required)</p>"},{"location":"resources/faq/#can-i-run-the-workshop-labs-on-my-laptop","title":"Can I run the workshop labs on my laptop?","text":"<p>Yes, most labs are designed to run on a standard laptop with: - 4+ CPU cores - 8GB+ RAM (16GB recommended) - Docker Desktop installed - Internet connection for pulling images</p> <p>Some advanced exercises may benefit from a GPU, but alternatives are provided.</p>"},{"location":"resources/faq/#will-i-be-able-to-deploy-my-own-models-after-this-workshop","title":"Will I be able to deploy my own models after this workshop?","text":"<p>Yes! By the end of this workshop, you'll have the knowledge to: - Package your own GenAI models in Docker containers - Deploy models using Docker's Model Runner - Set up monitoring and scaling for your models - Follow best practices for production deployment</p>"},{"location":"resources/faq/#technical-questions","title":"Technical Questions","text":""},{"location":"resources/faq/#how-do-i-handle-large-model-weights-with-docker","title":"How do I handle large model weights with Docker?","text":"<p>Large model weights can be managed in several ways: - Volume Mounting: Keep model weights in volumes rather than in images - Multi-stage Builds: Download weights during container build - Registry Extensions: Use Docker Hub or other registries designed for ML artifacts - External Storage: Use object storage like S3 and download at runtime</p>"},{"location":"resources/faq/#whats-the-difference-between-model-runner-and-mcp-toolkit","title":"What's the difference between Model Runner and MCP Toolkit?","text":"<ul> <li>Model Runner: Focused on efficiently running and serving AI models with optimized performance</li> <li>MCP Toolkit: A broader suite of tools for managing the entire lifecycle of containerized models, including deployment, monitoring, and scaling</li> </ul>"},{"location":"resources/faq/#how-do-i-scale-genai-models-for-production-traffic","title":"How do I scale GenAI models for production traffic?","text":"<p>Several strategies can be used: - Horizontal Scaling: Deploy multiple container instances behind a load balancer - Auto-scaling: Use orchestration platforms to scale based on metrics - Batching: Implement request batching for better throughput - Load Balancing: Distribute requests across multiple instances - Caching: Implement response caching for common requests</p>"},{"location":"resources/faq/#can-i-deploy-quantized-models-with-docker","title":"Can I deploy quantized models with Docker?","text":"<p>Yes! Docker works well with quantized models: - Format Support: Model Runner supports common quantized formats (INT8, FP16) - Performance Benefits: Quantized models require less memory and compute - Deployment Options: Various inference servers that support quantization can be containerized</p>"},{"location":"resources/faq/#docker-specific-questions","title":"Docker-Specific Questions","text":""},{"location":"resources/faq/#do-i-need-docker-desktop-or-can-i-use-docker-engine","title":"Do I need Docker Desktop or can I use Docker Engine?","text":"<p>Either works fine for this workshop: - Docker Desktop: Recommended for Windows and Mac users for ease of use - Docker Engine: Suitable for Linux users or server deployments</p>"},{"location":"resources/faq/#how-do-i-enable-gpu-support-in-docker","title":"How do I enable GPU support in Docker?","text":"<p>To enable GPU support: 1. Install the NVIDIA driver for your GPU 2. Install the NVIDIA Container Toolkit 3. Restart the Docker daemon 4. Use the <code>--gpus</code> flag when running containers: <code>docker run --gpus all ...</code></p>"},{"location":"resources/faq/#can-i-use-docker-compose-with-genai-applications","title":"Can I use Docker Compose with GenAI applications?","text":"<p>Absolutely! Docker Compose is excellent for: - Development Environments: Setting up complete development stacks - Multi-container Applications: Coordinating model servers with databases and APIs - Resource Configuration: Declaring GPU and memory requirements - Network Configuration: Setting up communication between services</p>"},{"location":"resources/faq/#contributing-and-support","title":"Contributing and Support","text":""},{"location":"resources/faq/#how-can-i-contribute-to-this-workshop","title":"How can I contribute to this workshop?","text":"<p>You can contribute in several ways: - Submit Issues: Report bugs or suggest improvements on GitHub - Pull Requests: Contribute code, documentation, or new labs - Share Knowledge: Help others in the community forums - Spread the Word: Share the workshop with colleagues and on social media</p>"},{"location":"resources/faq/#where-can-i-get-help-if-im-stuck","title":"Where can I get help if I'm stuck?","text":"<p>If you need assistance: - GitHub Issues: File an issue - Slack Community: Join the Collabnix Slack - Discussion Forums: Participate in workshop discussions - Community Resources: Check the Useful Links section</p>"},{"location":"resources/faq/#will-this-workshop-be-updated-regularly","title":"Will this workshop be updated regularly?","text":"<p>Yes! The GenAI and Docker landscapes evolve rapidly, and we're committed to keeping this workshop current with: - New Docker features for AI workloads - Emerging GenAI techniques and models - Updated deployment patterns and best practices - Additional labs and examples</p> <p>Keep an eye on the repository for updates, or star it to receive notifications.</p>"},{"location":"resources/links/","title":"Useful Links","text":"<p>This page provides a collection of valuable resources for learning more about Generative AI with Docker.</p>"},{"location":"resources/links/#official-documentation","title":"Official Documentation","text":""},{"location":"resources/links/#docker-resources","title":"Docker Resources","text":"<ul> <li>Docker Documentation</li> <li>Docker Hub</li> <li>Docker GitHub Repository</li> <li>Docker Blog</li> </ul>"},{"location":"resources/links/#genai-frameworks","title":"GenAI Frameworks","text":"<ul> <li>Hugging Face</li> <li>PyTorch</li> <li>TensorFlow</li> <li>ONNX Runtime</li> </ul>"},{"location":"resources/links/#tutorials-and-guides","title":"Tutorials and Guides","text":""},{"location":"resources/links/#docker-with-genai","title":"Docker with GenAI","text":"<ul> <li>Docker AI/ML Guide</li> <li>Running ML Models in Docker</li> <li>Managing GPU in Docker</li> <li>NVIDIA Container Toolkit</li> </ul>"},{"location":"resources/links/#genai-deployment","title":"GenAI Deployment","text":"<ul> <li>Hugging Face Model Deployment</li> <li>TensorFlow Serving</li> <li>ONNX Runtime Deployment</li> <li>Triton Inference Server</li> </ul>"},{"location":"resources/links/#community-resources","title":"Community Resources","text":""},{"location":"resources/links/#forums-and-communities","title":"Forums and Communities","text":"<ul> <li>Docker Forums</li> <li>Docker Community Slack</li> <li>Hugging Face Forums</li> <li>Reddit r/Docker</li> <li>Reddit r/MachineLearning</li> </ul>"},{"location":"resources/links/#blogs-and-publications","title":"Blogs and Publications","text":"<ul> <li>Towards Data Science</li> <li>ML Engineering Blog</li> <li>Collabnix Docker Blog</li> <li>ML-Ops.org</li> </ul>"},{"location":"resources/links/#learning-paths","title":"Learning Paths","text":""},{"location":"resources/links/#docker-learning","title":"Docker Learning","text":"<ul> <li>Docker Getting Started</li> <li>Play with Docker</li> <li>Docker Certified Associate</li> </ul>"},{"location":"resources/links/#genai-learning","title":"GenAI Learning","text":"<ul> <li>Hugging Face Course</li> <li>Practical Deep Learning for Coders</li> <li>MIT Introduction to Deep Learning</li> <li>Stanford CS224N: NLP with Deep Learning</li> </ul>"},{"location":"resources/links/#open-source-projects","title":"Open Source Projects","text":""},{"location":"resources/links/#docker-projects","title":"Docker Projects","text":"<ul> <li>Docker Compose</li> <li>Docker Machine</li> <li>Docker Swarm</li> </ul>"},{"location":"resources/links/#genai-projects","title":"GenAI Projects","text":"<ul> <li>Transformers</li> <li>LangChain</li> <li>Lit-LLM</li> <li>LlamaIndex</li> </ul>"},{"location":"resources/links/#books","title":"Books","text":""},{"location":"resources/links/#docker-books","title":"Docker Books","text":"<ul> <li>\"Docker: Up &amp; Running\" by Sean P. Kane and Karl Matthias</li> <li>\"Docker in Action\" by Jeff Nickoloff and Stephen Kuenzli</li> <li>\"Docker Deep Dive\" by Nigel Poulton</li> </ul>"},{"location":"resources/links/#genai-books","title":"GenAI Books","text":"<ul> <li>\"Natural Language Processing with Transformers\" by Lewis Tunstall, Leandro von Werra, and Thomas Wolf</li> <li>\"Deep Learning for Computer Vision\" by Rajalingappaa Shanmugamani</li> <li>\"Generative Deep Learning\" by David Foster</li> </ul>"},{"location":"resources/links/#videos-and-presentations","title":"Videos and Presentations","text":""},{"location":"resources/links/#docker-videos","title":"Docker Videos","text":"<ul> <li>Docker YouTube Channel</li> <li>DockerCon Presentations</li> </ul>"},{"location":"resources/links/#genai-videos","title":"GenAI Videos","text":"<ul> <li>Andrew Ng's Deep Learning Specialization</li> <li>Yannic Kilcher's YouTube Channel</li> <li>Two Minute Papers</li> </ul>"},{"location":"resources/links/#tools-and-utilities","title":"Tools and Utilities","text":""},{"location":"resources/links/#docker-tools","title":"Docker Tools","text":"<ul> <li>Portainer - Docker UI management</li> <li>dive - Docker image exploration tool</li> <li>ctop - Container metrics and monitoring</li> </ul>"},{"location":"resources/links/#genai-tools","title":"GenAI Tools","text":"<ul> <li>Weights &amp; Biases - Experiment tracking</li> <li>Gradio - ML demo interface builder</li> <li>LM Studio - Local LLM runner</li> <li>Ollama - Local LLM deployment tool</li> </ul>"},{"location":"resources/links/#workshops-and-training","title":"Workshops and Training","text":"<ul> <li>Docker Training</li> <li>DeepLearning.AI</li> <li>Full Stack Deep Learning</li> <li>Practical MLOps Workshop</li> </ul> <p>Remember to regularly check these resources as the fields of Docker and GenAI are rapidly evolving with new tools, techniques, and best practices emerging frequently.</p>"}]}